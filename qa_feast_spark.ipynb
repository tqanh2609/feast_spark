{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "msg_id": "c1dd1ac3-f7af-4059-b769-89efc6c0ebbb"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Define the list of packages to install\n",
    "packages = [\n",
    "    \"feast\",\n",
    "    \"feast[spark]\",\n",
    "    \"feast[singlestore]\",\n",
    "    \"pyarrow==19.0.1\"\n",
    "]\n",
    "\n",
    "# Construct the pip install command\n",
    "command = [sys.executable, \"-m\", \"pip\", \"install\"] + packages\n",
    "\n",
    "# Execute the command\n",
    "subprocess.check_call(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "535f6fa4-2165-4e59-9657-b67b7f6183fe"
   },
   "outputs": [],
   "source": [
    "### Diagnostics\n",
    "# SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "5bd45896-14a8-4c1a-a640-14232725a9ae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://spark-master-headless-e11ac1da-acf9-4b49-ae65-01b16243e812:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://jkg-deployment-e11ac1da-acf9-4b49-ae65-01b16243e812-564c84lqzhd:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>python3.11</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f34666aae10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global spark exists but not on local thread\n",
    "# SparkSession._instantiatedSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "53e4976c-40b1-43e0-ad95-d17f3f54d0b6"
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "msg_id": "d5d4cbf0-e317-419b-afe5-62873473c7e9"
   },
   "outputs": [],
   "source": [
    "# client\n",
    "wxd_hms_username = 'devadmin'\n",
    "wxd_hms_password = 'AX2Z1MBUoOergIb'\n",
    "\n",
    "# landingdev\n",
    "iceberg_catalog = \"thidiemcatalog\"\n",
    "access_key = \"WEeGX66cjJBVgira3DcU\"\n",
    "secret_key = \"BgbbrH08yKFcM4rz1RPUlRxWn3yuMV46W1nE0uTO\"\n",
    "bucket_name = \"thidiem\"\n",
    "hivemetastore_host = \"thrift://ibm-lh-lakehouse-hive-metastore-svc.zen.svc.cluster.local:9083\"\n",
    "\n",
    "s3_endpoint = \"https://rook-ceph-rgw.vnpt.vn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "msg_id": "716f8a2b-1f73-459e-b844-296919a1177a"
   },
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#             .appName(\"test\") \\\n",
    "#             .config(\"spark.datasource.singlestore.clientEndpoint\", \"192.168.0.121:32216\") \\\n",
    "#             .config(\"spark.datasource.singlestore.user\", \"admin\") \\\n",
    "#             .config(\"spark.datasource.singlestore.password\", \"secretpass\") \\\n",
    "#             .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "#             .config(\"spark.hive.metastore.use.SSL\", \"true\") \\\n",
    "#             .config(\"spark.hive.metastore.truststore.type\", \"JKS\") \\\n",
    "#             .config(\"spark.hive.metastore.truststore.path\", \"file:///opt/ibm/jdk/lib/security/cacerts\") \\\n",
    "#             .config(\"spark.hive.metastore.truststore.password\", \"changeit\") \\\n",
    "#             .config(\"spark.hive.metastore.client.auth.mode\", \"PLAIN\") \\\n",
    "#             .config(\"spark.hive.metastore.client.plain.username\", wxd_hms_username) \\\n",
    "#             .config(\"spark.hive.metastore.client.plain.password\", wxd_hms_password) \\\n",
    "#             .config(f\"spark.sql.catalog.{iceberg_catalog}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "#             .config(f\"spark.sql.catalog.{iceberg_catalog}.type\", \"hive\") \\\n",
    "#             .config(f\"spark.sql.catalog.{iceberg_catalog}.uri\", hivemetastore_host) \\\n",
    "#             .config(f\"spark.hadoop.fs.s3a.bucket.{bucket_name}.endpoint\", s3_endpoint) \\\n",
    "#             .config(f\"spark.hadoop.fs.s3a.bucket.{bucket_name}.access.key\", access_key) \\\n",
    "#             .config(f\"spark.hadoop.fs.s3a.bucket.{bucket_name}.secret.key\", secret_key) \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\") \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.endpoint\", s3_endpoint) \\\n",
    "#             .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "#             .config(\"spark.executor.extraJavaOptions\", \"-Daws.java.v1.disableDeprecationAnnouncement=true\") \\\n",
    "#             .config(\"spark.driver.extraJavaOptions\", \"-Daws.java.v1.disableDeprecationAnnouncement=true\") \\\n",
    "#             .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "21f7bdeb-2e9a-47f0-9ffe-37a68ad86571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feast in ./python/lib/python/site-packages (0.41.3)\n",
      "Requirement already satisfied: pyarrow==19.0.1 in ./python/lib/python/site-packages (19.0.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in ./python/lib/python/site-packages (from feast) (8.1.7)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /cc-home/_global_/python-3 (from feast) (0.4.6)\n",
      "Requirement already satisfied: dill~=0.3.0 in ./python/lib/python/site-packages (from feast) (0.3.7)\n",
      "Requirement already satisfied: protobuf<5.0.0,>=4.24.0 in ./python/lib/python/site-packages (from feast) (4.25.7)\n",
      "Requirement already satisfied: Jinja2<4,>=2 in /cc-home/_global_/python-3 (from feast) (3.1.6)\n",
      "Requirement already satisfied: jsonschema in ./python/lib/python/site-packages (from feast) (4.19.2)\n",
      "Requirement already satisfied: mmh3 in ./python/lib/python/site-packages (from feast) (5.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.22 in ./python/lib/python/site-packages (from feast) (1.26.4)\n",
      "Requirement already satisfied: pandas<3,>=1.4.3 in ./python/lib/python/site-packages (from feast) (2.1.4)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in ./python/lib/python/site-packages (from feast) (2.5.3)\n",
      "Requirement already satisfied: pygments<3,>=2.12.0 in /cc-home/_global_/python-3 (from feast) (2.19.1)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.0 in /cc-home/_global_/python-3 (from feast) (6.0.2)\n",
      "Requirement already satisfied: requests in /cc-home/_global_/python-3 (from feast) (2.32.3)\n",
      "Requirement already satisfied: SQLAlchemy>1 in ./python/lib/python/site-packages (from SQLAlchemy[mypy]>1->feast) (2.0.25)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.0 in ./python/lib/python/site-packages (from feast) (0.8.10)\n",
      "Requirement already satisfied: tenacity<9,>=7 in ./python/lib/python/site-packages (from feast) (8.2.2)\n",
      "Requirement already satisfied: toml<1,>=0.10.0 in ./python/lib/python/site-packages (from feast) (0.10.2)\n",
      "Requirement already satisfied: tqdm<5,>=4 in ./python/lib/python/site-packages (from feast) (4.65.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./python/lib/python/site-packages (from feast) (4.4.2)\n",
      "Requirement already satisfied: fastapi>=0.68.0 in ./python/lib/python/site-packages (from feast) (0.115.12)\n",
      "Requirement already satisfied: uvicorn<1,>=0.14.0 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.34.2)\n",
      "Requirement already satisfied: uvicorn-worker in ./python/lib/python/site-packages (from feast) (0.3.0)\n",
      "Requirement already satisfied: dask>=2024.2.1 in ./python/lib/python/site-packages (from dask[dataframe]>=2024.2.1->feast) (2025.4.1)\n",
      "Requirement already satisfied: prometheus-client in ./python/lib/python/site-packages (from feast) (0.14.1)\n",
      "Requirement already satisfied: psutil in ./python/lib/python/site-packages (from feast) (5.9.0)\n",
      "Requirement already satisfied: bigtree>=0.19.2 in ./python/lib/python/site-packages (from feast) (0.28.0)\n",
      "Requirement already satisfied: pyjwt in ./python/lib/python/site-packages (from feast) (2.4.0)\n",
      "Requirement already satisfied: gunicorn in ./python/lib/python/site-packages (from feast) (23.0.0)\n",
      "Requirement already satisfied: pyspark<4,>=3.0.0 in ./python/lib/python/site-packages (from feast[spark]) (3.5.5)\n",
      "Requirement already satisfied: singlestoredb in ./python/lib/python/site-packages (from feast[singlestore]) (1.13.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (3.1.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (23.1)\n",
      "Requirement already satisfied: partd>=1.4.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (0.12.0)\n",
      "Requirement already satisfied: importlib_metadata>=4.13.0 in ./python/lib/python/site-packages (from dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (7.0.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in ./python/lib/python/site-packages (from fastapi>=0.68.0->feast) (0.46.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./python/lib/python/site-packages (from fastapi>=0.68.0->feast) (4.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./python/lib/python/site-packages (from Jinja2<4,>=2->feast) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./python/lib/python/site-packages (from pandas<3,>=1.4.3->feast) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /cc-home/_global_/python-3 (from pandas<3,>=1.4.3->feast) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./python/lib/python/site-packages (from pandas<3,>=1.4.3->feast) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./python/lib/python/site-packages (from pydantic>=2.0.0->feast) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in ./python/lib/python/site-packages (from pydantic>=2.0.0->feast) (2.14.6)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./python/lib/python/site-packages (from pyspark<4,>=3.0.0->feast[spark]) (0.10.9.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from SQLAlchemy>1->SQLAlchemy[mypy]>1->feast) (3.0.1)\n",
      "Requirement already satisfied: mypy>=0.910 in ./python/lib/python/site-packages (from SQLAlchemy[mypy]>1->feast) (1.15.0)\n",
      "Requirement already satisfied: h11>=0.8 in ./python/lib/python/site-packages (from uvicorn<1,>=0.14.0->uvicorn[standard]<1,>=0.14.0->feast) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (1.1.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./python/lib/python/site-packages (from uvicorn[standard]<1,>=0.14.0->feast) (15.0.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in ./python/lib/python/site-packages (from jsonschema->feast) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./python/lib/python/site-packages (from jsonschema->feast) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./python/lib/python/site-packages (from jsonschema->feast) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./python/lib/python/site-packages (from jsonschema->feast) (0.10.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests->feast) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests->feast) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests->feast) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests->feast) (2023.7.22)\n",
      "Requirement already satisfied: build in ./python/lib/python/site-packages (from singlestoredb->feast[singlestore]) (1.2.2.post1)\n",
      "Requirement already satisfied: parsimonious in ./python/lib/python/site-packages (from singlestoredb->feast[singlestore]) (0.10.0)\n",
      "Requirement already satisfied: setuptools in ./python/lib/python/site-packages (from singlestoredb->feast[singlestore]) (72.1.0)\n",
      "Requirement already satisfied: sqlparams in ./python/lib/python/site-packages (from singlestoredb->feast[singlestore]) (6.2.0)\n",
      "Requirement already satisfied: wheel in ./python/lib/python/site-packages (from singlestoredb->feast[singlestore]) (0.41.2)\n",
      "Requirement already satisfied: zipp>=0.5 in ./python/lib/python/site-packages (from importlib_metadata>=4.13.0->dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (3.17.0)\n",
      "Requirement already satisfied: mypy_extensions>=1.0.0 in ./python/lib/python/site-packages (from mypy>=0.910->SQLAlchemy[mypy]>1->feast) (1.0.0)\n",
      "Requirement already satisfied: locket in ./python/lib/python/site-packages (from partd>=1.4.0->dask>=2024.2.1->dask[dataframe]>=2024.2.1->feast) (1.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /cc-home/_global_/python-3 (from python-dateutil>=2.8.2->pandas<3,>=1.4.3->feast) (1.17.0)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in ./python/lib/python/site-packages (from starlette<0.47.0,>=0.40.0->fastapi>=0.68.0->feast) (4.9.0)\n",
      "Requirement already satisfied: pyproject_hooks in ./python/lib/python/site-packages (from build->singlestoredb->feast[singlestore]) (1.2.0)\n",
      "Requirement already satisfied: regex>=2022.3.15 in ./python/lib/python/site-packages (from parsimonious->singlestoredb->feast[singlestore]) (2024.11.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./python/lib/python/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi>=0.68.0->feast) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install feast feast[spark] feast[singlestore] pyarrow==19.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# navigate file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "39e2c221-fd9c-45e8-960a-4d0efb8911f1"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# from ibm_watson_studio_lib import access_project_or_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "ea8cbb0c-7d17-45e3-a5e0-a7d0ffd96b41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/project_data/data_asset/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wslib = access_project_or_space()\n",
    "# base_dir = wslib.mount.get_base_dir()\n",
    "# base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "c01e24e9-f628-4630-b5e3-a82e0b83d291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1625180\n",
      "-rw-rw-r--. 1 1000750000 1000750000       965 Mar  4 07:44  54dantoc.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000      1494 Mar 18 07:55  abstract_test.py\n",
      "-rw-rw-r--. 1 1000750000 1000750000   1510880 Jan 17 02:16  BankChurners.csv\n",
      "drwxrws---. 4 1000750000 1000750000        12 Dec  3 19:09  \u001b[0m\u001b[01;34mboto3-1.35.74\u001b[0m/\n",
      "-rw-rw-r--. 1 1000750000 1000750000    211301 Dec  6 04:18  commons-dbcp2-2.9.0.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000    145516 Dec  6 04:26  commons-pool2-2.11.1.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000      4092 Apr 21 03:53  converter-jackson-2.9.0.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000      9001 Jan 16 07:55 'Credit Score Classification Dataset.csv'\n",
      "-rw-r--r--. 1 1000750000 1000750000    803710 Apr 18 01:59 'CTDL - Con người.xlsx'\n",
      "drwxr-sr-x. 2 1000750000 1000750000         1 Apr 24 03:07  \u001b[01;34mdata\u001b[0m/\n",
      "-rw-r--r--. 1 1000750000 1000750000   1004706 Apr 21 02:14  data_21.xlsx\n",
      "-rw-r--r--. 1 1000750000 1000750000    962655 Apr 20 13:30  data.xlsx\n",
      "-rw-r--r--. 1 1000750000 1000750000      8312 Apr 21 03:40  dbnd-api-deequ-1.0.28.1.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000    176051 Apr 21 03:45  dbnd-client-1.0.28.1.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000     35083 Apr 23 11:09  driver_stats.parquet\n",
      "-rw-rw-r--. 1 1000750000 1000750000      6855 Feb 20 08:54  dvc_dm_ngaynghile_202408211453.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000  26666090 Mar 31 10:31  dvc_hf_dataset_202411131103.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000     21640 Dec 20 08:41  dvc_nofeatures.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000 122267180 Mar  4 01:55  DVHC.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000      1702 Mar 25 02:40  es-cert.p12\n",
      "-rw-rw-r--. 1 1000750000 1000750000      1854 Mar 11 02:32  es-cert.pem\n",
      "-rw-r--r--. 1 1000750000 1000750000      5786 Apr 24 02:51  example_repo.py\n",
      "drwxr-sr-x. 2 1000750000 1000750000         1 Apr 24 07:52  \u001b[01;34mfeast_test\u001b[0m/\n",
      "-rw-r--r--. 1 1000750000 1000750000    854388 Apr 11 01:50  final_item_feature.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000   3275823 Apr 11 01:50  final_training_sample.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000   8666638 Apr 11 01:50  final_user_feature.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000      3162 Mar 26 09:47  gidac_xml_mapping.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    803710 Apr 18 02:00  human_data.xlsx\n",
      "-rw-r--r--. 1 1000750000 1000750000         6 Apr 24 03:02  __init__.py\n",
      "-rw-rw-r--. 1 1000750000 1000750000    641587 Dec  6 04:24  mariadb-java-client-3.1.4.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000 262131325 Jan 16 09:07 'neo_bank_churn (1).csv'\n",
      "-rw-rw-r--. 1 1000750000 1000750000  33444886 Jan 17 02:06 'neo_bank_churn (2).csv'\n",
      "-rw-rw-r--. 1 1000750000 1000750000 422254185 Jan 17 06:52  neo_bank_churn.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    782603 Apr 21 03:53  okhttp-4.10.0.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000      2016 Feb 24 07:38  presto_engine_cert.crt\n",
      "-rw-rw-r--. 1 1000750000 1000750000      2016 Feb 24 07:05  presto_engine_cert.pem\n",
      "-rw-rw-r--. 1 1000750000 1000750000      2388 Feb 24 07:38  presto_instance_cert.crt\n",
      "-rw-rw-r--. 1 1000750000 1000750000      2388 Feb 24 07:24  presto_instance_cert.pem\n",
      "-rw-rw-r--. 1 1000750000 1000750000  15659856 Mar  4 01:21  quan_ly_doi_tuong_tra_cuu_da_chieu111.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000  15659856 Mar  4 01:10  quan_ly_doi_tuong_tra_cuu_da_chieu.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000  17955686 Mar  3 10:34  quan_ly_doi_tuong_tra_cuu_da_chieu.xlsx\n",
      "-rw-r--r--. 1 1000750000 1000750000    854388 Apr 15 07:43  recsys_item_feature.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000   3148736 Apr 15 07:43  recsys_training_sample.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000   7196381 Apr 15 07:43  recsys_user_feature.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    231062 Mar 27 09:17  regression.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    125435 Apr 21 03:53  retrofit-2.9.0.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000     31939 Mar 18 02:38  sample_submission.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000  14612772 Feb 21 08:04  sentiment_sum.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000    671043 Dec  6 04:19  singlestore-jdbc-client-1.2.3.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000    511977 Dec  6 04:17  singlestore-spark-connector_2.12-4.1.6-spark-3.4.2.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000    742292 Mar 11 04:23  s_pakn_forecasting_v1.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000   6162507 Feb 27 08:44  spark-excel_2.12-0.13.7.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000  30930051 Apr 18 02:19  spark-excel_2.12-3.5.0_0.20.3.jar\n",
      "-rw-r--r--. 1 1000750000 1000750000    153612 Mar 24 03:22  spark-xml_2.12-0.18.0.jar\n",
      "-rw-rw-r--. 1 1000750000 1000750000 121800373 Jan 16 03:41  store_sales_forecasting.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    451405 Mar 18 02:38  test.csv\n",
      "-rw-rw-r--. 1 1000750000 1000750000        47 Dec  6 04:21  test_job_1733393139245.txt\n",
      "-rw-r--r--. 1 1000750000 1000750000      4378 Apr 24 02:51  test_workflow.py\n",
      "-rw-rw-r--. 1 1000750000 1000750000 540091925 Feb 28 04:37  Thy_best_model_state.bin\n",
      "-rw-rw-r--. 1 1000750000 1000750000       220 Mar  5 01:28  tongiao.csv\n",
      "-rw-r--r--. 1 1000750000 1000750000    460676 Mar 18 02:38  train.csv\n"
     ]
    }
   ],
   "source": [
    "# ls -l /project_data/data_asset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "msg_id": "80d3c0af-daf2-46ed-bf4c-7d9d516df716"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder: /project_data/data_asset/feast_test\n"
     ]
    }
   ],
   "source": [
    "# # 1. Create a new subfolder\n",
    "# new_folder = os.path.join(base_dir, 'feast_test')\n",
    "# os.makedirs(new_folder, exist_ok=True)\n",
    "# print(f\"Created folder: {new_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "msg_id": "5aacb06f-659a-4669-80a7-03d5951a9592"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project_data/data_asset/boto3-1.35.74/boto3/__init__.py\n",
      "/project_data/data_asset/boto3-1.35.74/boto3/resources/__init__.py\n",
      "/project_data/data_asset/boto3-1.35.74/boto3/docs/__init__.py\n",
      "/project_data/data_asset/boto3-1.35.74/boto3/s3/__init__.py\n",
      "/project_data/data_asset/boto3-1.35.74/boto3/dynamodb/__init__.py\n",
      "/project_data/data_asset/boto3-1.35.74/boto3/ec2/__init__.py\n",
      "/project_data/data_asset/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# # tìm file path\n",
    "# !find /project_data/data_asset/ | grep init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "msg_id": "7ce21257-e2eb-4d0b-80f1-a43d84bf1600"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved /project_data/data_asset/__init__.py to /project_data/data_asset/feast_test/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# # 2. Move a file into the new subfolder\n",
    "# src_file = os.path.join(base_dir, '__init__.py')\n",
    "# dst_file = os.path.join(new_folder, '__init__.py')\n",
    "# shutil.move(src_file, dst_file)\n",
    "# print(f\"Moved {src_file} to {dst_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "msg_id": "fb56550f-f6a4-4ee4-b83a-f1f694e4f5bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project_data/data_asset/feast_test contains dirs [] and files ['feature_store.yaml', 'example_repo.py', '__init__.py', 'test_workflow.py']\n"
     ]
    }
   ],
   "source": [
    "# # 3. List contents\n",
    "# for root, dirs, files in os.walk(new_folder):\n",
    "#     print(root, 'contains dirs', dirs, 'and files', files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "msg_id": "de8b796d-0ee4-430d-ac3f-e5c97c582e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== File Contents ===\n",
      "#dummy\n"
     ]
    }
   ],
   "source": [
    "# # đọc content của file\n",
    "# def read_py_file(path):\n",
    "#     \"\"\"\n",
    "#     Reads a Python (.py) file from the given path and returns its contents as a string.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         with open(path, 'r', encoding='utf-8') as f:\n",
    "#             content = f.read()\n",
    "#         return content\n",
    "#     except FileNotFoundError:\n",
    "#         print(f\"File not found: {path}\")\n",
    "#     except IOError as e:\n",
    "#         print(f\"I/O error({e.errno}): {e.strerror}\")\n",
    "\n",
    "# # Example usage:\n",
    "# file_path = '/project_data/data_asset/feast_test/__init__.py'\n",
    "# script_content = read_py_file(file_path)\n",
    "# if script_content is not None:\n",
    "#     print(\"=== File Contents ===\")\n",
    "#     print(script_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "msg_id": "ba6cdc40-dde1-4680-81f8-a904ec14df30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project_data/data_asset/feast_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/shared/python/lib/python/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# cd /project_data/data_asset/feast_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "msg_id": "5efdca2f-5a00-4993-9a15-85088a6a5801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/            feature_store.yaml  test_download.parquet\n",
      "example_repo.py  __init__.py         test_workflow.py\n"
     ]
    }
   ],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "msg_id": "60866a13-205b-4380-be43-36b6c6c1a903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ibm/spark/python/lib/pyspark.zip/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "\n",
      "--- Run feast apply ---\n",
      "/opt/ibm/spark/python/lib/pyspark.zip/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "Applying changes for project test_feast_ibm\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_store.py:580: RuntimeWarning: On demand feature view is an experimental feature. This API is stable, but the functionality does not scale well for offline retrieval\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats_fresh\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "\n",
      "--- Historical features for training ---\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:132: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/project_data/data_asset/feast_test/test_workflow.py\", line 130, in <module>\n",
      "    run_demo()\n",
      "  File \"/project_data/data_asset/feast_test/test_workflow.py\", line 16, in run_demo\n",
      "    fetch_historical_features_entity_df(store, for_batch_scoring=False)\n",
      "  File \"/project_data/data_asset/feast_test/test_workflow.py\", line 84, in fetch_historical_features_entity_df\n",
      "    training_df = store.get_historical_features(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/feature_store.py\", line 1163, in get_historical_features\n",
      "    job = provider.get_historical_features(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/passthrough_provider.py\", line 428, in get_historical_features\n",
      "    job = self.offline_store.get_historical_features(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py\", line 138, in get_historical_features\n",
      "    spark_session = get_spark_session_or_start_new_with_repoconfig(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py\", line 448, in get_spark_session_or_start_new_with_repoconfig\n",
      "    spark_session = spark_builder.getOrCreate()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 477, in getOrCreate\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 512, in getOrCreate\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 200, in __init__\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 287, in _do_init\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 417, in _initialize_context\n",
      "  File \"/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1587, in __call__\n",
      "  File \"/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:839)\n",
      "\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n",
      "\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:839)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !python test_workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read from mount volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "d67a6947-58ba-43e0-b0c4-9bd8c0efda7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnts/code/QuangAnh/feastspark/main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/shared/python/lib/python/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "# cd /mnts/code/QuangAnh/feastspark/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "bd36bedc-9874-40d5-a68a-4b6c83455b11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mdata\u001b[0m/            feature_store.yaml  \u001b[01;34m__pycache__\u001b[0m/      Untitled-1.ipynb\n",
      "example_repo.py  __init__.py         test_workflow.py\n"
     ]
    }
   ],
   "source": [
    "# ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "msg_id": "9ba42d3f-fd92-4437-9d46-d2ffdb997ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/ibm/spark/python/lib/pyspark.zip/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "\n",
      "--- Run feast apply to setup feature store on Snowflake ---\n",
      "/opt/ibm/spark/python/lib/pyspark.zip/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "No project found in the repository. Using project name test_feast_ibm defined in feature_store.yaml\n",
      "Applying changes for project test_feast_ibm\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: customer_daily_profile registered entities. Please check if you have applied your entities correctly.Entities: ['customer'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: driver_hourly_stats registered entities. Please check if you have applied your entities correctly.Entities: ['driver'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mcustomer_daily_profile\u001b[0m\n",
      "Deploying infrastructure for \u001b[1m\u001b[32mdriver_hourly_stats\u001b[0m\n",
      "\n",
      "--- Historical features for training ---\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: driver_hourly_stats registered entities. Please check if you have applied your entities correctly.Entities: ['driver'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:132: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnts/code/QuangAnh/feastspark/main/test_workflow.py\", line 124, in <module>\n",
      "    run_demo()\n",
      "  File \"/mnts/code/QuangAnh/feastspark/main/test_workflow.py\", line 16, in run_demo\n",
      "    fetch_historical_features_entity_df(store, for_batch_scoring=False)\n",
      "  File \"/mnts/code/QuangAnh/feastspark/main/test_workflow.py\", line 79, in fetch_historical_features_entity_df\n",
      "    training_df = store.get_historical_features(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/feature_store.py\", line 1163, in get_historical_features\n",
      "    job = provider.get_historical_features(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/passthrough_provider.py\", line 428, in get_historical_features\n",
      "    job = self.offline_store.get_historical_features(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py\", line 138, in get_historical_features\n",
      "    spark_session = get_spark_session_or_start_new_with_repoconfig(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py\", line 448, in get_spark_session_or_start_new_with_repoconfig\n",
      "    spark_session = spark_builder.getOrCreate()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 477, in getOrCreate\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 512, in getOrCreate\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 200, in __init__\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 287, in _do_init\n",
      "  File \"/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py\", line 417, in _initialize_context\n",
      "  File \"/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1587, in __call__\n",
      "  File \"/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n",
      ": org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.base/java.lang.Thread.run(Thread.java:839)\n",
      "\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n",
      "\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:839)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !python test_workflow.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path you want to change to\n",
    "path = '/mnts/code/QuangAnh/feastspark/main'\n",
    "\n",
    "# Change the current working directory\n",
    "os.chdir(path)\n",
    "\n",
    "# Optional: Verify the change\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "msg_id": "e3f31463-d8a1-4360-8db7-e060ca47853d"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from feast import FeatureStore\n",
    "from feast.data_source import PushMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "msg_id": "a00e2470-bf1e-4133-a03f-37fb6947299d"
   },
   "outputs": [],
   "source": [
    "def fetch_historical_features_entity_df(store: FeatureStore, for_batch_scoring: bool):\n",
    "    # Note: see https://docs.feast.dev/getting-started/concepts/feature-retrieval for more details on how to retrieve\n",
    "    # for all entities in the offline store instead\n",
    "    entity_df = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            # entity's join key -> entity values\n",
    "            \"driver_id\": [1001, 1002, 1003],\n",
    "            # \"event_timestamp\" (reserved key) -> timestamps\n",
    "            \"event_timestamp\": [\n",
    "                datetime(2021, 4, 12, 10, 59, 42),\n",
    "                datetime(2021, 4, 12, 8, 12, 10),\n",
    "                datetime(2021, 4, 12, 16, 40, 26),\n",
    "            ],\n",
    "            # (optional) label name -> label values. Feast does not process these\n",
    "            \"label_driver_reported_satisfaction\": [1, 5, 3],\n",
    "            # values we're using for an on-demand transformation\n",
    "            \"val_to_add\": [1, 2, 3],\n",
    "            \"val_to_add_2\": [10, 20, 30],\n",
    "        }\n",
    "    )\n",
    "    # For batch scoring, we want the latest timestamps\n",
    "    if for_batch_scoring:\n",
    "        entity_df[\"event_timestamp\"] = pd.to_datetime(\"now\", utc=True)\n",
    "\n",
    "    training_df = store.get_historical_features(\n",
    "        entity_df=entity_df,\n",
    "        features=[\n",
    "            \"driver_hourly_stats:conv_rate\",\n",
    "            \"driver_hourly_stats:acc_rate\",\n",
    "            \"driver_hourly_stats:avg_daily_trips\",\n",
    "            # \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "            # \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "        ],\n",
    "    ).to_df()\n",
    "    print(training_df.head())\n",
    "\n",
    "\n",
    "def fetch_online_features(store, use_feature_service: bool):\n",
    "    entity_rows = [\n",
    "        # {join_key: entity_value}\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "            \"val_to_add\": 1000,\n",
    "            \"val_to_add_2\": 2000,\n",
    "        },\n",
    "        {\n",
    "            \"driver_id\": 1002,\n",
    "            \"val_to_add\": 1001,\n",
    "            \"val_to_add_2\": 2002,\n",
    "        },\n",
    "    ]\n",
    "    if use_feature_service:\n",
    "        features_to_fetch = store.get_feature_service(\"driver_activity_v1\")\n",
    "    else:\n",
    "        features_to_fetch = [\n",
    "            \"driver_hourly_stats:acc_rate\",\n",
    "            \"driver_hourly_stats:avg_daily_trips\",\n",
    "            # \"transformed_conv_rate:conv_rate_plus_val1\",\n",
    "            # \"transformed_conv_rate:conv_rate_plus_val2\",\n",
    "        ]\n",
    "    returned_features = store.get_online_features(\n",
    "        features=features_to_fetch,\n",
    "        entity_rows=entity_rows,\n",
    "    ).to_dict()\n",
    "    for key, value in sorted(returned_features.items()):\n",
    "        print(key, \" : \", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "msg_id": "cc8bf626-ad23-4153-88db-8c101d3c40de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Run feast apply ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ibm/spark/python/lib/pyspark.zip/pyspark/broadcast.py:38: DeprecationWarning: typing.io is deprecated, import directly from typing instead. typing.io will be removed in Python 3.12.\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No project found in the repository. Using project name test_feast_ibm defined in feature_store.yaml\n",
      "Applying changes for project test_feast_ibm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: customer_daily_profile registered entities. Please check if you have applied your entities correctly.Entities: ['customer'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: driver_hourly_stats registered entities. Please check if you have applied your entities correctly.Entities: ['driver'] vs Entity Columns: []\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying infrastructure for customer_daily_profile\n",
      "Deploying infrastructure for driver_hourly_stats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['feast', 'apply'], returncode=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = FeatureStore(repo_path=\".\")\n",
    "print(\"\\n--- Run feast apply ---\")\n",
    "subprocess.run([\"feast\", \"apply\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "msg_id": "6a9e9ffe-c0c1-4e17-a512-3bcdf0a490bf"
   },
   "outputs": [],
   "source": [
    "### Diagnostics\n",
    "SparkSession.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "msg_id": "4e4649d0-5e07-40e8-a767-bb4c91568bc4"
   },
   "outputs": [],
   "source": [
    "# global spark exists but not on local thread\n",
    "SparkSession._instantiatedSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "msg_id": "848ee49f-2b33-41f1-a157-650a4864b30a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active SparkSession found.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    # Attempt to retrieve the existing SparkSession\n",
    "    spark = SparkSession.getActiveSession()\n",
    "    if spark is not None:\n",
    "        # Check if the associated SparkContext is active\n",
    "        if not spark.sparkContext._jsc.sc().isStopped():\n",
    "            print(\"An active SparkContext is currently running.\")\n",
    "        else:\n",
    "            print(\"SparkContext exists but is stopped.\")\n",
    "    else:\n",
    "        print(\"No active SparkSession found.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking SparkContext: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "msg_id": "a4cc2a17-50cf-4959-8a02-db7900c3a775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No active SparkContext found.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Check if default SparkContext exists\n",
    "sc = SparkContext._active_spark_context\n",
    "if sc is not None:\n",
    "    print(f\"Active SparkContext found: {sc.appName}\")\n",
    "    print(f\"SparkContext is stopped: {sc._jsc.sc().isStopped()}\")\n",
    "else:\n",
    "    print(\"No active SparkContext found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "msg_id": "a8f5bf0c-8912-40fc-bbed-1127ef51e435"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Historical features for training ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: driver_hourly_stats registered entities. Please check if you have applied your entities correctly.Entities: ['driver'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:132: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:839)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:839)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Historical features for training ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m fetch_historical_features_entity_df(store, for_batch_scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m, in \u001b[0;36mfetch_historical_features_entity_df\u001b[0;34m(store, for_batch_scoring)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m for_batch_scoring:\n\u001b[1;32m     23\u001b[0m     entity_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow\u001b[39m\u001b[38;5;124m\"\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m training_df \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m     26\u001b[0m     entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m     27\u001b[0m     features\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:conv_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:acc_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:avg_daily_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# \"transformed_conv_rate:conv_rate_plus_val1\",\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# \"transformed_conv_rate:conv_rate_plus_val2\",\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     ],\n\u001b[1;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mto_df()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/feature_store.py:1163\u001b[0m, in \u001b[0;36mFeatureStore.get_historical_features\u001b[0;34m(self, entity_df, features, full_feature_names)\u001b[0m\n\u001b[1;32m   1160\u001b[0m utils\u001b[38;5;241m.\u001b[39m_validate_feature_refs(_feature_refs, full_feature_names)\n\u001b[1;32m   1161\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_provider()\n\u001b[0;32m-> 1163\u001b[0m job \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   1165\u001b[0m     feature_views,\n\u001b[1;32m   1166\u001b[0m     _feature_refs,\n\u001b[1;32m   1167\u001b[0m     entity_df,\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry,\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject,\n\u001b[1;32m   1170\u001b[0m     full_feature_names,\n\u001b[1;32m   1171\u001b[0m )\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/passthrough_provider.py:428\u001b[0m, in \u001b[0;36mPassthroughProvider.get_historical_features\u001b[0;34m(self, config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_historical_features\u001b[39m(\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    420\u001b[0m     config: RepoConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     full_feature_names: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RetrievalJob:\n\u001b[0;32m--> 428\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m    429\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    430\u001b[0m         feature_views\u001b[38;5;241m=\u001b[39mfeature_views,\n\u001b[1;32m    431\u001b[0m         feature_refs\u001b[38;5;241m=\u001b[39mfeature_refs,\n\u001b[1;32m    432\u001b[0m         entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m    433\u001b[0m         registry\u001b[38;5;241m=\u001b[39mregistry,\n\u001b[1;32m    434\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m    435\u001b[0m         full_feature_names\u001b[38;5;241m=\u001b[39mfull_feature_names,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:138\u001b[0m, in \u001b[0;36mSparkOfflineStore.get_historical_features\u001b[0;34m(config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fv\u001b[38;5;241m.\u001b[39mbatch_source, SparkSource)\n\u001b[1;32m    132\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe spark offline store is an experimental feature in alpha development. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome functionality may still be unstable so functionality can change in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m spark_session \u001b[38;5;241m=\u001b[39m get_spark_session_or_start_new_with_repoconfig(\n\u001b[1;32m    139\u001b[0m     store_config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39moffline_store\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m tmp_entity_df_table_name \u001b[38;5;241m=\u001b[39m offline_utils\u001b[38;5;241m.\u001b[39mget_temp_entity_table_name()\n\u001b[1;32m    143\u001b[0m entity_schema \u001b[38;5;241m=\u001b[39m _get_entity_schema(\n\u001b[1;32m    144\u001b[0m     spark_session\u001b[38;5;241m=\u001b[39mspark_session,\n\u001b[1;32m    145\u001b[0m     entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:448\u001b[0m, in \u001b[0;36mget_spark_session_or_start_new_with_repoconfig\u001b[0;34m(store_config)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spark_conf:\n\u001b[1;32m    444\u001b[0m         spark_builder \u001b[38;5;241m=\u001b[39m spark_builder\u001b[38;5;241m.\u001b[39mconfig(\n\u001b[1;32m    445\u001b[0m             conf\u001b[38;5;241m=\u001b[39mSparkConf()\u001b[38;5;241m.\u001b[39msetAll([(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m spark_conf\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    446\u001b[0m         )\n\u001b[0;32m--> 448\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m spark_builder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    449\u001b[0m spark_session\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.parser.quotedRegexColumnNames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[1;32m    203\u001b[0m         sparkHome,\n\u001b[1;32m    204\u001b[0m         pyFiles,\n\u001b[1;32m    205\u001b[0m         environment,\n\u001b[1;32m    206\u001b[0m         batchSize,\n\u001b[1;32m    207\u001b[0m         serializer,\n\u001b[1;32m    208\u001b[0m         conf,\n\u001b[1;32m    209\u001b[0m         jsc,\n\u001b[1;32m    210\u001b[0m         profiler_cls,\n\u001b[1;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:839)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:839)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Historical features for training ---\")\n",
    "fetch_historical_features_entity_df(store, for_batch_scoring=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "msg_id": "b059033f-7b9d-455b-b8b9-18a6152018d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Historical features for batch scoring ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark_source.py:64: RuntimeWarning: The spark data source API is an experimental feature in alpha development. This API is unstable and it could and most probably will be changed in the future.\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/feature_view.py:430: UserWarning: There are some mismatches in your feature view: driver_hourly_stats registered entities. Please check if you have applied your entities correctly.Entities: ['driver'] vs Entity Columns: []\n",
      "  warnings.warn(\n",
      "/home/spark/shared/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:132: RuntimeWarning: The spark offline store is an experimental feature in alpha development. Some functionality may still be unstable so functionality can change in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:839)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:839)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Historical features for batch scoring ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m fetch_historical_features_entity_df(store, for_batch_scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m, in \u001b[0;36mfetch_historical_features_entity_df\u001b[0;34m(store, for_batch_scoring)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m for_batch_scoring:\n\u001b[1;32m     23\u001b[0m     entity_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevent_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnow\u001b[39m\u001b[38;5;124m\"\u001b[39m, utc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m training_df \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m     26\u001b[0m     entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m     27\u001b[0m     features\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:conv_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:acc_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdriver_hourly_stats:avg_daily_trips\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;66;03m# \"transformed_conv_rate:conv_rate_plus_val1\",\u001b[39;00m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;66;03m# \"transformed_conv_rate:conv_rate_plus_val2\",\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     ],\n\u001b[1;32m     34\u001b[0m )\u001b[38;5;241m.\u001b[39mto_df()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(training_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/feature_store.py:1163\u001b[0m, in \u001b[0;36mFeatureStore.get_historical_features\u001b[0;34m(self, entity_df, features, full_feature_names)\u001b[0m\n\u001b[1;32m   1160\u001b[0m utils\u001b[38;5;241m.\u001b[39m_validate_feature_refs(_feature_refs, full_feature_names)\n\u001b[1;32m   1161\u001b[0m provider \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_provider()\n\u001b[0;32m-> 1163\u001b[0m job \u001b[38;5;241m=\u001b[39m provider\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m   1165\u001b[0m     feature_views,\n\u001b[1;32m   1166\u001b[0m     _feature_refs,\n\u001b[1;32m   1167\u001b[0m     entity_df,\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry,\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject,\n\u001b[1;32m   1170\u001b[0m     full_feature_names,\n\u001b[1;32m   1171\u001b[0m )\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/passthrough_provider.py:428\u001b[0m, in \u001b[0;36mPassthroughProvider.get_historical_features\u001b[0;34m(self, config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_historical_features\u001b[39m(\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    420\u001b[0m     config: RepoConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     full_feature_names: \u001b[38;5;28mbool\u001b[39m,\n\u001b[1;32m    427\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RetrievalJob:\n\u001b[0;32m--> 428\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffline_store\u001b[38;5;241m.\u001b[39mget_historical_features(\n\u001b[1;32m    429\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    430\u001b[0m         feature_views\u001b[38;5;241m=\u001b[39mfeature_views,\n\u001b[1;32m    431\u001b[0m         feature_refs\u001b[38;5;241m=\u001b[39mfeature_refs,\n\u001b[1;32m    432\u001b[0m         entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m    433\u001b[0m         registry\u001b[38;5;241m=\u001b[39mregistry,\n\u001b[1;32m    434\u001b[0m         project\u001b[38;5;241m=\u001b[39mproject,\n\u001b[1;32m    435\u001b[0m         full_feature_names\u001b[38;5;241m=\u001b[39mfull_feature_names,\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m job\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:138\u001b[0m, in \u001b[0;36mSparkOfflineStore.get_historical_features\u001b[0;34m(config, feature_views, feature_refs, entity_df, registry, project, full_feature_names)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fv\u001b[38;5;241m.\u001b[39mbatch_source, SparkSource)\n\u001b[1;32m    132\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe spark offline store is an experimental feature in alpha development. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome functionality may still be unstable so functionality can change in the future.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    136\u001b[0m )\n\u001b[0;32m--> 138\u001b[0m spark_session \u001b[38;5;241m=\u001b[39m get_spark_session_or_start_new_with_repoconfig(\n\u001b[1;32m    139\u001b[0m     store_config\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39moffline_store\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m tmp_entity_df_table_name \u001b[38;5;241m=\u001b[39m offline_utils\u001b[38;5;241m.\u001b[39mget_temp_entity_table_name()\n\u001b[1;32m    143\u001b[0m entity_schema \u001b[38;5;241m=\u001b[39m _get_entity_schema(\n\u001b[1;32m    144\u001b[0m     spark_session\u001b[38;5;241m=\u001b[39mspark_session,\n\u001b[1;32m    145\u001b[0m     entity_df\u001b[38;5;241m=\u001b[39mentity_df,\n\u001b[1;32m    146\u001b[0m )\n",
      "File \u001b[0;32m~/python/lib/python/site-packages/feast/infra/offline_stores/contrib/spark_offline_store/spark.py:448\u001b[0m, in \u001b[0;36mget_spark_session_or_start_new_with_repoconfig\u001b[0;34m(store_config)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m spark_conf:\n\u001b[1;32m    444\u001b[0m         spark_builder \u001b[38;5;241m=\u001b[39m spark_builder\u001b[38;5;241m.\u001b[39mconfig(\n\u001b[1;32m    445\u001b[0m             conf\u001b[38;5;241m=\u001b[39mSparkConf()\u001b[38;5;241m.\u001b[39msetAll([(k, v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m spark_conf\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m    446\u001b[0m         )\n\u001b[0;32m--> 448\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m spark_builder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m    449\u001b[0m spark_session\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.parser.quotedRegexColumnNames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spark_session\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate(sparkConf)\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    198\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[1;32m    203\u001b[0m         sparkHome,\n\u001b[1;32m    204\u001b[0m         pyFiles,\n\u001b[1;32m    205\u001b[0m         environment,\n\u001b[1;32m    206\u001b[0m         batchSize,\n\u001b[1;32m    207\u001b[0m         serializer,\n\u001b[1;32m    208\u001b[0m         conf,\n\u001b[1;32m    209\u001b[0m         jsc,\n\u001b[1;32m    210\u001b[0m         profiler_cls,\n\u001b[1;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvironment[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPYTHONHASHSEED\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc \u001b[38;5;241m=\u001b[39m jsc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_context(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\u001b[38;5;241m.\u001b[39m_jconf)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m SparkConf(_jconf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39mconf())\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[0;34m(self, jconf)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;124;03mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mJavaSparkContext(jconf)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1581\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1582\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1583\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1584\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1586\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1587\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1588\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[1;32m   1590\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1591\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Only one SparkContext should be running in this JVM (see SPARK-2243).The currently running SparkContext was created at:\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\njava.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\njava.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.base/java.lang.Thread.run(Thread.java:839)\n\tat org.apache.spark.SparkContext$.$anonfun$assertNoOtherContextIsRunning$2(SparkContext.scala:2697)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.SparkContext$.assertNoOtherContextIsRunning(SparkContext.scala:2694)\n\tat org.apache.spark.SparkContext$.markPartiallyConstructed(SparkContext.scala:2784)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:97)\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:238)\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:839)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Historical features for batch scoring ---\")\n",
    "fetch_historical_features_entity_df(store, for_batch_scoring=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Load features into online store ---\")\n",
    "store.materialize_incremental(end_date=datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Online features ---\")\n",
    "fetch_online_features(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 with Spark",
   "language": "python3",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
